# PageIndex - Hierarchical Document Retrieval

Vectorless, reasoning-based RAG using hierarchical tree indexing. Inspired by [VectifyAI/PageIndex](https://github.com/VectifyAI/PageIndex).

Instead of chunk-embed-search, PageIndex parses a document's Table of Contents into a tree with LLM-generated summaries, then navigates the tree through reasoning to find relevant sections.

## How It Works

**Indexing** — Parse the PDF's TOC with an LLM, generate summaries for each section in parallel (`Task.async_stream`), and build a tree:

```
Document PDF → TocParser (LLM) → Section tree → Parallel summarization → Index JSON
```

**Retrieval** — Four modes, from simple to autonomous:

| Mode | How it works | Best for |
|------|-------------|----------|
| `--simple` | Score all nodes, fetch top-k, synthesize | Predictable, single-hop lookups |
| (default) | Single SubAgent with `get-content`/`grep-content` tools | Most questions |
| `--planner` | MetaPlanner decomposes into fetch/compute/synthesize tasks | Multi-hop, computation-heavy |
| `--iterative` | Two-agent extraction/synthesis loop with structured findings | Adaptive multi-step retrieval |

**Default agent mode** runs a single multi-turn LLM conversation that iteratively fetches sections and synthesizes an answer.

**Planner mode** generates a structured plan with separate fetch, compute, and synthesize tasks executed in dependency order, with optional quality gates and replanning. Fetches run in parallel without LLM calls (direct PTC-Lisp), while analysis and synthesis use dedicated SubAgents.

**Iterative mode** separates data extraction from reasoning. An extraction agent fetches sections and returns structured findings with provenance (page, section, context). A synthesis agent evaluates whether the findings are sufficient to answer, or requests one more "shopping item" to search for. The loop continues until an answer is produced or max iterations are reached. See `iterative_retrieval_design.md` for the full design rationale.

## Setup

```bash
cd examples/page_index
mix deps.get
mix download              # Download sample PDFs (~8 MB)
```

## Usage

```bash
# Index a document (parses TOC, generates summaries)
mix run run.exs --index data/3M_2022_10K.pdf

# Query with default agent mode
mix run run.exs --query "What are 3M's business segments?" --pdf data/3M_2022_10K.pdf

# Query with simple scoring
mix run run.exs --query "What drove operating margin change?" --pdf data/3M_2022_10K.pdf --simple

# Query with MetaPlanner (for complex multi-hop questions)
mix run run.exs --query "Is 3M a capital intensive business?" --pdf data/3M_2022_10K.pdf --planner --trace

# Query with iterative extraction/synthesis loop
mix run run.exs --query "Which segment dragged down growth?" --pdf data/3M_2022_10K.pdf --iterative --trace

# Show an existing index
mix run run.exs --show data/3M_2022_10K_index.json
```

Options: `--model <name>` (default: `bedrock:haiku`), `--trace` (writes to `traces/`), `--simple`, `--planner`, `--iterative`.

Short aliases: `-m`, `-q`, `-s`, `-p`, `-i`, `-t`.

## Benchmarking

```bash
# Run benchmark (default: haiku+sonnet, agent+planner modes, 3M_2022_10K questions)
mix run bench.exs --runs 1

# Specific models and modes
mix run bench.exs --runs 3 --models bedrock:haiku --modes agent,iterative

# Full test set
mix run bench.exs --test-set full --runs 1

# Analyze latest benchmark with LLM-as-judge
mix run analyze.exs --judge --model bedrock:sonnet
```

Benchmark options: `--runs N`, `--test-set NAME`, `--question ID`, `--models LIST`, `--modes LIST`, `--self-failure`.

Results are written to `bench_runs/<timestamp>/` with a manifest and per-run traces.

## Tools

All retrieval modes provide these tools to their agents:

**`get-content` / `fetch_section`** — Fetch document content by section node_id with offset-based pagination. Returns content (5000-6000 chars per call), truncation status, and continuation hints. Supports fuzzy matching on node_id.

**`grep-content` / `grep_section`** — Search section content for keywords/patterns before fetching. Returns up to 5 matches with surrounding context and offset hints. Enables a grep-then-fetch pattern to avoid reading entire large sections.

## Example: PTC-Lisp Programs Generated by Agents

All agent programs are written in PTC-Lisp by the LLM. Below are real examples from benchmark traces.

**Grep-then-fetch** — The agent searches for keywords before fetching full content:

```clojure
(def grep_result (tool/grep_section {:node_id "management_s_di_performance_by_business_segmen"
                                     :pattern "organic"}))
(println "Matches:" (count (:matches grep_result)))
(doseq [m (:matches grep_result)]
  (println "Offset:" (:offset m) "Context:" (:context m)))
```

**Fetch with pagination** — When content is truncated, the agent continues at the next offset:

```clojure
(def section (tool/fetch_section {:node_id "management_s_di_results_of_operations" :offset 0}))
(println "Truncated:" (get section "truncated") "Total:" (get section "total_chars"))

;; Continue reading from where we left off
(def section2 (tool/fetch_section {:node_id "management_s_di_results_of_operations"
                                   :offset 5000}))
```

**Extraction agent return** — Structured findings with provenance (iterative mode):

```clojure
(return {:findings [
  {:label "operating_margin_2022" :value 19.1 :unit "percent"
   :page 27 :section "Results of Operations"
   :context "Operating income margin for FY2022"}
  {:label "operating_margin_2021" :value 20.8 :unit "percent"
   :page 27 :section "Results of Operations"
   :context "Operating income margin for FY2021"}
  {:label "cost_of_sales_change" :value 3.0 :unit "percent"
   :page 27 :section "Results of Operations"
   :context "Increase in cost of sales, driven by PFAS litigation and manufacturing exit costs"}
  {:label "combat_arms_litigation_charge" :value 1200 :unit "millions_usd"
   :page 27 :section "Results of Operations"
   :context "Q2 2022 pre-tax charge for Combat Arms Earplugs litigation"}]
 :sections_searched ["management_s_di_results_of_operations"
                     "management_s_di_overview"]})
```

**Computation task** — Planner mode generates ratio calculations from fetched data (Q3: "Is 3M capital-intensive?"):

```clojure
(let [capex_2022 1749.0
      revenue_2022 34229.0
      ppe_net_2022 9178.0
      total_assets_2022 46455.0
      depreciation_2022 1831.0
      capex_to_revenue_pct (* 100.0 (/ capex_2022 revenue_2022))
      ppe_to_assets_pct (* 100.0 (/ ppe_net_2022 total_assets_2022))
      depreciation_to_revenue_pct (* 100.0 (/ depreciation_2022 revenue_2022))]
  (return {:capex_to_revenue_pct capex_to_revenue_pct
           :ppe_to_assets_pct ppe_to_assets_pct
           :depreciation_to_revenue_pct depreciation_to_revenue_pct}))
```

**Cross-task data access** — Planner agents read results from earlier tasks via `data/results`:

```clojure
(let [segments (get-in data/results ["fetch_segment_growth_data" "segments"])
      overall  (get-in data/results ["fetch_overall_growth" "overall_organic_growth_pct"])]
  (return {:overall_organic_pct overall
           :segments (map (fn [seg]
                            {:name (get seg "name")
                             :organic_growth_pct (get seg "organic_growth_pct")
                             :delta (- (get seg "organic_growth_pct") overall)})
                          segments)}))
```

Note: The current benchmark questions are mostly retrieval-heavy — the LLM needs to find the right sections and extract data, but rarely needs complex computation. Only Q3 ("Is 3M capital-intensive?") triggers ratio calculations. Most programs are tool calls, pagination, and structured returns. More computation-heavy questions (e.g., multi-year trend analysis, cross-document comparisons) would exercise the PTC-Lisp arithmetic more heavily.

## Example: MetaPlanner Generated Plan

In planner mode, `PlanExecutor.run/2` sends a **mission** (the question + a list of available document sections with summaries) and **constraints** (the required JSON structure with one example plan) to the LLM. The LLM generates the entire plan — agent definitions, prompts, task decomposition, signatures, dependencies, and verification expressions — from scratch each time. The only guidance is:

- Use a `document_analyst` agent (with `fetch_section`/`grep_section` tools) for data extraction
- Use `(fail "reason")` when data isn't found
- Create computation agents (no tools) if ratios or derived values are needed
- End with a `synthesis_gate` task with id `"final_answer"`

The agent prompts (e.g., "You are a quantitative analyst...") are **invented by the planner LLM**, not hardcoded. This means different questions produce different agent configurations — Q3 below gets a `calculator` agent, while Q2 (segment growth) gets a `growth_calculator` with a different prompt.

Below is a real plan generated for Q3: *"Is 3M a capital-intensive business based on FY2022 data?"* The four `fetch_*` tasks have no dependencies and run in parallel. The `calculator` waits for all fetches, then computes ratios. The `synthesizer` produces the final answer.

```json
{
  "agents": {
    "calculator": {
      "prompt": "You are a quantitative analyst. Extract numeric values into let bindings and use arithmetic expressions (/, *, +, -) to compute ratios. Do NOT calculate values mentally — write the expressions and let the interpreter compute them.",
      "tools": []
    },
    "document_analyst": {
      "prompt": "You are a data extraction agent. Use grep_section first to locate keywords, then fetch_section at the returned offset. When truncated: true, paginate. If the data is not found, call (fail \"reason\").",
      "tools": ["fetch_section", "grep_section"]
    },
    "synthesizer": {
      "prompt": "You produce clear, evidence-based answers from structured data. For capital intensity questions, discuss the computed ratios and provide a definitive answer supported by the numbers.",
      "tools": []
    }
  },
  "tasks": [
    {
      "id": "fetch_balance_sheet",
      "agent": "document_analyst",
      "depends_on": [],
      "input": "Fetch 'financial_state_consolidated_balance_sheet_at_' and extract: total PP&E (net) and total assets for FY2022.",
      "signature": "{ppe_net_2022 :float, total_assets_2022 :float, page :int}"
    },
    {
      "id": "fetch_cash_flow",
      "agent": "document_analyst",
      "depends_on": [],
      "input": "Fetch 'financial_state_consolidated_statement_of_cash' and extract: capital expenditures for FY2022, FY2021, and FY2020.",
      "signature": "{capex_2022 :float, capex_2021 :float, capex_2020 :float, page :int}"
    },
    {
      "id": "fetch_income_statement",
      "agent": "document_analyst",
      "depends_on": [],
      "input": "Fetch 'financial_state_consolidated_statement_of_inco' and extract: total net sales for FY2022, FY2021, and FY2020.",
      "signature": "{revenue_2022 :float, revenue_2021 :float, revenue_2020 :float, page :int}"
    },
    {
      "id": "fetch_depreciation",
      "agent": "document_analyst",
      "depends_on": [],
      "input": "Fetch 'notes_to_consol_note_7_supplemental_balance_sh' and extract: depreciation expense for FY2022.",
      "signature": "{depreciation_2022 :float, page :int}"
    },
    {
      "id": "compute_capital_intensity_metrics",
      "agent": "calculator",
      "depends_on": ["fetch_balance_sheet", "fetch_cash_flow", "fetch_income_statement", "fetch_depreciation"],
      "input": "Calculate: (1) capex/revenue %, (2) PP&E/assets %, (3) avg capex/revenue over 2020-2022, (4) depreciation/revenue %",
      "output": "ptc_lisp",
      "signature": "{capex_to_revenue_2022_pct :float, ppe_to_assets_2022_pct :float, avg_capex_to_revenue_pct :float}",
      "verification": "(and (number? (get data/result \"capex_to_revenue_2022_pct\")) (number? (get data/result \"ppe_to_assets_2022_pct\")))"
    },
    {
      "id": "final_answer",
      "type": "synthesis_gate",
      "agent": "synthesizer",
      "depends_on": ["compute_capital_intensity_metrics", "fetch_balance_sheet", "fetch_cash_flow", "fetch_income_statement"],
      "input": "Based on the computed metrics, determine whether 3M is a capital-intensive business. Provide a clear yes/no answer with supporting evidence.",
      "signature": "{is_capital_intensive :bool, rationale :string, key_metrics {capex_to_revenue_pct :float, ppe_to_assets_pct :float}}"
    }
  ]
}
```

Key plan features:
- **Parallel fetches**: the four `fetch_*` tasks have `depends_on: []` and execute concurrently
- **Typed signatures**: each task declares its output shape so downstream tasks know what to expect
- **Verification expressions**: PTC-Lisp predicates that validate task results before passing them downstream
- **`output: "ptc_lisp"`**: tells the calculator agent to write a PTC-Lisp program rather than free-text
- **`synthesis_gate`**: marks `final_answer` as the terminal task — the plan succeeds when it returns

## Architecture

```
lib/page_index/
├── parser.ex              # PDF text extraction (via pdfplumber)
├── toc_parser.ex          # LLM-based TOC parsing
├── fine_indexer.ex         # Index builder (TOC → summaries → tree)
├── retriever.ex           # Agent-based and simple retrieval
├── planner_retriever.ex   # MetaPlanner-based multi-hop retrieval
├── iterative_retriever.ex # Two-agent iterative extraction/synthesis loop
└── plan_trace.ex          # File-based JSONL tracing for PlanExecutor events
```

## Iterative Retriever Design

The iterative mode addresses limitations of the other modes:

- **Agent mode** conflates fetching, extraction, reasoning, and synthesis in one agent. Raw PDF text accumulates in context across turns (~80k tokens).
- **Planner mode** predicts which sections to fetch upfront before seeing any data, leading to brittle plans and harmful quality gate rejections.

The iterative approach separates extraction from reasoning using two agents in a loop:

```
Loop state: findings [] + failed_searches []

         ┌───────────────────────--──────────────┐
         │        Loop (max 4 iterations)        │
         │                                       │
         │  ┌──────────────┐   ┌─────────────┐   │
 shopping │  │  Extraction │──→│  Synthesis  │   │
   item ──│─→│    Agent    │   │    Agent    │   │
         │  └──────────────┘   └─────────────┘   │
         │   multi-turn,         single-turn,    │
         │   has tools           no tools        │
         │                         │             │
         │              "answer" ──│──→ done     │
         │              "needs"  ──│──→ next iter │
         │              "fail"   ──│──→ done     │
         └──────────────────────────────────────┘
```

**Extraction agent** (multi-turn SubAgent, up to 10 turns):
- **Input**: a "shopping item" (what to search for) + the original question + section index
- **Tools**: `fetch_section`, `grep_section` — same pagination and fuzzy matching as other modes
- **Output**: structured findings list, each with `{label, value, unit, page, section, context}`
- **On failure**: `(fail "reason")` — recorded in `failed_searches` so the synthesis agent knows what was already tried
- **Context**: fresh each iteration — no conversation history from previous rounds

Signature:

```
(shopping_item :string, question :string) ->
  {findings    [{label :string, value :any, unit :string,
                 page :any, section :string, context :string}],
   sections_searched [:string]}
```

The prompt instructs the agent to extract ALL relevant data points (not just the shopping item target), use raw numeric values, always note the scale from table headers, and paginate truncated sections rather than guessing from partial data. The `context` field captures what each value represents so the synthesis agent can reason about it without seeing the raw PDF text.

**Synthesis agent** (single-turn SubAgent, no tools):
- **Input**: the question + all accumulated findings (formatted as a numbered list) + all failed searches
- **Output**: one of three statuses:
  - `"answer"` — sufficient data, returns the answer with cited page numbers
  - `"needs"` — requests one specific next shopping item (e.g., "Consumer segment operating income for 2022")
  - `"fail"` — data genuinely insufficient, no alternative paths
- **Key property**: never sees raw PDF text — only the structured findings from extraction agents

Signature:

```
{status :string, answer :string, sources [:string],
 confidence :string, needs :string, reason :string}
```

The prompt tells the synthesis agent to perform computations itself using the findings values (checking unit consistency), to be specific in `needs` requests (not vague "segment data" but "Consumer segment operating income for 2022"), and to never repeat a failed search. On the final iteration, a `last_chance` flag instructs it to answer with whatever data is available rather than requesting more.

Unlike the planner mode, both prompts are **hardcoded** — there is no planning LLM that invents agents or task structure. The loop logic is in Elixir code (`Enum.reduce_while`), not generated by an LLM.

**Loop flow**:
1. Initial shopping item is the question itself
2. Extraction agent fetches sections, returns findings with provenance
3. Synthesis agent evaluates: answer, request more data, or fail
4. If "needs": the requested item becomes the next shopping item, loop continues
5. Findings accumulate across iterations (append-only), failed searches are tracked
6. On max iterations: one final "last chance" synthesis attempt with all collected data

This design keeps token costs moderate (~15-25k vs ~80k for agent mode) because raw PDF text is discarded after each extraction — only the structured findings survive.

## Known Limitation: Planner Interpretation Instability

The planner mode reliably extracts consistent numbers but the synthesis step is unstable for subjective questions like "Is 3M capital-intensive?" — different runs pick different metrics and thresholds, leading to contradictory conclusions from identical data.

## References

- [VectifyAI/PageIndex](https://github.com/VectifyAI/PageIndex) — Original Python implementation
- [FinanceBench](https://github.com/patronus-ai/financebench) — Source of test questions (`data/questions.json`, MIT license)
- `iterative_retrieval_design.md` — Full design document for the iterative retriever
- `bench_results.md` — Latest benchmark results
